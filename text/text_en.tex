\documentclass[10pt]{article}
\usepackage[czech]{babel}
\usepackage[utf8]{inputenc}
\begin{document}
\title{Crawlcheck - specification}
\author{Alex Mansurov}
\maketitle
\newpage
\section{Problem definition}
\paragraph{Problem set}
Crawlcheck - pre-release website checker - the foundation can be HTML and CSS syntax verification and link checking. Similar software already exist, however only few solutions respect contemporary web standards and is effective enough for verification of a large web project. 
\paragraph{List of requirements}
\begin{itemize}
	\item Proxy
	\item Validators and log
	\item Crawler - limited for domain or list of domains
	\item Configuration language with rules - incl. exclude (eg. directory), implicit settings (eg. don't trace links), activation of in-depth checking for given path
	\item Human readable output - parametrized views, cluster where are errors and what passed the tests.
	\item Cache
\end{itemize}
\paragraph{Inputs} Configuration settings and list of verified websites form the input.
\paragraph{Outputs} Output is list of syntax errors and incorrect links, incl. unavailable scripts and multimedia on tested sites.
\paragraph{User tasks} User navigates the web in their browser, using the proxy server interface. Verification is done on the background. User receives response from the web server and can manually check appearance, content correctness, as well as functionality of client side scripts. User can also access report interface to see results of verification and add annotation or comment. \section{Architecture specification - first version}
\paragraph{Construction blocks}
\begin{enumerate}
	\item Proxy
	\item Analyzer
	\item Report
	\item Crawler
	\item Dispatcher
	\item Configurator
\end{enumerate}
\paragraph{Proxy} Proxy server is the main user interface. The tool is expected to be used for one large website verification at one moment. There can be several links leading to external sites, as well as parts located on custom subdomains. There can also be various client-side scripts affecting appearance and functionality.
\paragraph{~} The first idea - that user provides an entry point from which the web will be automatically crawled in depth is not suitable, since detailed analysis of scripts for link detection, as well as search depth control would be required, so as to cover most of the actual website, as well as not to waste resources to explore external websites.
\paragraph{~} On the other hand, during testing or verification of a website prior to final release, user - tester - assistance might be suitable. They can verify additional requirements that are hard to test automatically - client-side scripts functionality, content correctness and visual requirements.
\paragraph{~} For these reasons, intermediary module between client and web server option was chosen. It automatically does syntax, link existence and possibly other checking on background. In addition, limited scope depth searches are possible, keeping track of what pages were previewed to the tester in browser.
\paragraph{~} Proxy module communicates with user and web server, transfers requests from user to web server and responses from web server back to user. In additoon, data are transfered to other modules for application's own activities. 
\paragraph{~}Error handling. In case of HTTP error defined by error code, is such reported "as is" (delivered to user and recorded). In case of lost connection with user, while still having active connection with web server,  discovered links are still processed. In case of lost connection with web server, cached data can be delivered to user, in case those are not available, HTTP error is returned.
\paragraph{~}Multithreading on web server side. Communication with web server is  multithreaded. In each thread individual connection is conducted. Threads load addresses from joint pool where are located requests from both user and Crawler module.
\paragraph{~}Single user vs. multi user. The nature of the application expects processing one request from user at one moment. Therefore, on client side, proxy works in one thread with sequential processing. Alternatively, processing can be expanded in a similar way as the server side is being done, if in future that becomes necessary. It's expected that more traffic comes from Crawler module rather than from client.
\paragraph{Analyzer} Analyzer is the key module of the application. It takes care about verification of data coming from web server and handles results to report module. The actual verification is being done by specialized "submodules" These could be plugins with common interface, what allows extending application with additional forms of verification in future.
\paragraph{~}World wide web consorcium (W3C) defines set of standards  for modern web. \footnote{http://www.w3.org/standards/} Also it provides HTML validator written in Perl\footnote{http://validator.w3.org/source/}, CSS validator written in Java\footnote{https://jigsaw.w3.org/css-validator/documentation.html} as well as other validators. \footnote{http://www.w3.org/QA/Tools/} The licence allows free modification and distribution of modified version.\footnote{http://www.w3.org/Consortium/Legal/2002/copyright-software-20021231} Existing validators could be used in application. W3C also provides commercial tool Validator Suite (TM) offering similar functionality as this application.
\paragraph{~}Plugin interface
\paragraph{~}Scripts interoperability
\paragraph{~}Error handling. If there is no plugin for particular type of data or an error happens during during the analysis, data are ignored and not handled to the report module. If dispatcher module or report module's database isn't working, analysis is not being done.
\paragraph{Report} Report module stores results of verification received from analyzer module and provides acces to them. Graphical user interface for results preview is defined. Discovered links are available for in-depth scan. Interface for findings annotation by tester is defined. Tester can add own comments for each finding. This mainly includes manual verification discoveries and notes - errors in content, appearance, scripts functionality, etc. Possible "false positive" findings can be marked. Apart of interactive interface report module allows to generate a text report with stored findings. 
\paragraph{~}Report content
\paragraph{~}Findings storage
\paragraph{~}Data security
\paragraph{~}Interface for interacting with other modules
\paragraph{~}Interactive application functionaluty
\paragraph{~}Localisation
\paragraph{~}Error handling. If database is not working, analysis nor crawling are working, and report access is not available. If report preview or text document generation is not working, the rest of application can continue and data will be available later.
\paragraph{Crawler} Crawler module is taking detecting links from report, which are later downloaded and subject to separate analysis, unless it was already verified before.
\paragraph{Dispatcher} Dispatcher module controls the data flow. For request it decides if analysis is necessary or if it was already done. If analysis is necessary, the data are handled from proxy module to analyzer module. It handles data from crawler module to proxy module and assures response to them will not reach the client. Checks, that request from client gets the top priority and balances the load of proxy from crawler.
\paragraph{~} Responses from server are collected in cache. When requested content is already stored, response is not outdated and it was not an error previously, return cached content to client. Otherwise, the data will be fetched and subject to analysis.
\paragraph{~} Error handling. On error in dispatcher modules proxy, crawler and analyzer stops as well as they are managed by the dispatcher. Report module keeps working, allowing to preview and anntoate findings and generate text documents.
\paragraph{~} Data formats for communication between modules.
\paragraph{Configurator} Configurator module cares about loading user configuration. Based on that, parameters for individual modules are set. Configuration is loaded on application start.
\paragraph{~}Configuration parameters
\paragraph{~}Existing solutions for configuration
\paragraph{~}Error handling. If no configuration file exists, implicit configuration is used. If there's an error in configuration, the application is not started.
%% správa paměti - cache, report
%% výkon
\section{Architecture specification revisited}
\paragraph{Motivation} Other point of view on the architecture is from the data viewpoint. Initial specification implies two sort of data in the system: netwirk data - HTTP requests and responses and metadata - findings, errors, annotations etc.
\paragraph{Construction blocks}
\begin{itemize}
	\item Proxy
	\item Analyzer
	\item Crawler
	\item Report
	\item Configurator
\end{itemize}
\paragraph{~}Modules have similar functionality, however they differ in the way they communicate. Network data are kept in proxy module. Findings are kept in report module. Analyzer module works as an adapter between proxy and reportand Crawler module as an adapter from crawler to proxy. Structurally, Analyzer is consumer of HTTP reposnses produced by Proxy. Crawler is producent of HTTP requests for Proxy. Dispatcher module is simplified into particular communication interfaces among modules. Cache is closely tied to Proxy. Modules working with network and modules working with findings are separated.
\paragraph{~}Proxy module consists of interface communicating with client, interface communicating with server and middle interface handling data - among the two communication interfaces also to/from the rest of application.
\paragraph{~}Report module is composed of the database of collected findings with their user-added annotation and communication interfaces - facades for Crawler and Analyzer and complex interface for tester to preview and annotate findings. Internal architecture is Model-(View)-Controller.
\section{Existing tools}
\begin{itemize}
	\item W3C validator suite
	\item Fiddler - "free web debugging proxy for any browser, system or platform"
	\item Firebug - "Firebug integrates with Firefox to put a wealth of web development tools at your fingertips while you browse. You can edit, debug and monitor CSS, HTML and JavaScript live in any web page"
	\item Selenium - "portable software testing framework for web-applications"
	\item Watir - "web application testing in Ruby"
\end{itemize}	
\end{document}